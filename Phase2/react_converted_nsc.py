# %%
import os
# Azure OpenAI í™˜ê²½ ì„¤ì •
os.environ['OPENAI_API_VERSION'] = '2024-02-15-preview'
os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://upappliance.openai.azure.com'
os.environ['AZURE_OPENAI_API_KEY'] = '4gWgDnxVjVaIRQhPJFn2seBhFGhsgfZhpedg6J0lmWDdsltOAlD8JQQJ99AKACYeBjFXJ3w3AAABACOGylYc'
os.environ['AZURE_OPENAI_API_DEPLOYMENT_NAME'] ="gpt-4o-mini"

# -*- coding: utf-8 -*-
# =========================================================
# ICC / RAG / Self-Consistency (Top-2) Pipeline
# - ìš”ì²­ì‚¬í•­: sc_prompt / choose_prompt ê²°ê³¼ë¥¼ "íŒŒì‹± ì—†ì´" ì›ë¬¸ ê·¸ëŒ€ë¡œ ì €ì¥
# - Role/Task ë¬¸êµ¬ëŠ” ìœ ì§€, ì¶œë ¥ í¬ë§·ë§Œ JSON(Top-2)ìœ¼ë¡œ ìœ ë„í•˜ë˜ ì½”ë“œì—ì„œëŠ” íŒŒì‹±í•˜ì§€ ì•ŠìŒ
# =========================================================

# In[1]: ê¸°ë³¸ í™˜ê²½ ì„¤ì • --------------------------------------------------------
import os, sys, re, json, time, math
import pandas as pd

# (ê¶Œì¥) í™˜ê²½ë³€ìˆ˜ëŠ” OS í™˜ê²½ì— ë¯¸ë¦¬ ì„¸íŒ…í•˜ì„¸ìš”. ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë§Œ ë‚¨ê¹ë‹ˆë‹¤.
# os.environ['OPENAI_API_VERSION'] = '2024-02-15-preview'
# os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://<your-endpoint>.openai.azure.com'
# os.environ['AZURE_OPENAI_API_KEY'] = '<your-key>'
# os.environ['AZURE_OPENAI_API_DEPLOYMENT_NAME'] ="gpt-4o-mini"

# ì„ íƒ: ChromaDB ì‚¬ìš© ì‹œ í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
os.environ["CHROMA_TELEMETRY"] = "False"

import pysqlite3  # chroma í™˜ê²½ì—ì„œ sqlite ëŒ€ì²´ ì‚¬ìš© ì‹œ
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import chromadb
from chromadb import PersistentClient
from chromadb.config import Settings

from langchain_community.chat_models import ChatOpenAI as LegacyChatOpenAI  # (ë¯¸ì‚¬ìš©)
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.schema.runnable import RunnableSequence, RunnableMap, RunnableLambda
from langchain.callbacks import get_openai_callback
from langchain.schema import Document

# In[2]: íƒ€ì´ë° ë°ì½”ë ˆì´í„° ------------------------------------------------------
timing_results = []
def timed(name):
    def wrapper(fn):
        def inner(x):
            print(f"â±ï¸ [{name}] ì‹œì‘")
            start = time.time()
            result = fn(x)
            end = time.time()
            duration = end - start
            print(f"âœ… [{name}] ì™„ë£Œ - ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
            global timing_results
            timing_results.append((name, duration))
            return result
        return inner
    return wrapper

# In[3]: ë°ì´í„° ë¡œë“œ -----------------------------------------------------------
# df = pd.read_csv('../../../flow/1_ê°œë°œí´ë”_í•œì–‘ëŒ€/Phase2_RAG/train_set.csv', encoding='utf-8-sig')
df = pd.read_csv('/datasets/DTS2025000169/data/trainset_group.csv', encoding='utf-8-sig')

# ICC ê²°ê³¼ ì €ì¥ìš©(ì˜µì…˜)
icc_df = pd.DataFrame(columns=["ticket_id", "components", "before_change", "after_change", "ICC"])

# In[4]: LLM ì¸ìŠ¤í„´ìŠ¤ ----------------------------------------------------------
# LangChainì˜ AzureChatOpenAI ì‚¬ìš© (chat.completions í˜¸í™˜)
llm = AzureChatOpenAI(
    azure_deployment=os.environ.get("AZURE_OPENAI_API_DEPLOYMENT_NAME", "gpt-4o-mini"),
    openai_api_version=os.environ.get("OPENAI_API_VERSION", "2024-02-15-preview"),
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

# In[5]: ì„ë² ë”© & ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ ------------------------------------------
aoai_embeddings = AzureOpenAIEmbeddings(
    azure_deployment="text-embedding-3-small",
    openai_api_version=os.environ.get("OPENAI_API_VERSION", "2024-02-15-preview"),
)

# (ì´ˆê¸° êµ¬ì¶•ì€ ë³„ë„ í•¨ìˆ˜ë¡œ ì‹¤í–‰ í›„, ì—¬ê¸°ì„œëŠ” PersistentClientë¡œ ë¡œë“œ)
client = PersistentClient(path="./chromadb_store_1")
native_collection = client.get_collection("my_native_collection")

# ticket_id -> keyword ë§¤í•‘ (ìˆìœ¼ë©´ ì‚¬ìš©)
try:
    TICKET_TO_KEYWORD = dict(zip(df["ticket_id_hashed"], df.get("keyword", pd.Series(["N/A"] * len(df)))))
except Exception:
    TICKET_TO_KEYWORD = {}

# In[6]: í”„ë¡¬í”„íŠ¸ ì •ì˜ ----------------------------------------------------------

# (1) ICC ë¶„ë¥˜: ì˜ˆì‹œ/íŒë³„ê¸°ì¤€ ê°•í™”. ì¶œë ¥ì€ "Suggestion" ë˜ëŠ” "ICC" í•œ ë‹¨ì–´
icc_prompt = PromptTemplate.from_template("""
<Role> You are an expert in reading and classifying product feedback.</Role>

<Task>
Decide whether the user feedback is a "Suggestion" (new feature request / improvement) or "ICC".
- If the feedback includes **new feature requests or improvement suggestions**, classify as "Suggestion".
- Otherwise, if it fits Issues/Complaints/Comments (ICC), classify as "ICC".
</Task>

<Definitions with Examples>
[Issues (ì˜¤ë¥˜/ê³ ì¥/ê²°í•¨/ì—ëŸ¬)]
- Signals: "ì˜¤ë¥˜", "ê³ ì¥", "ì—ëŸ¬", "ì—…ë°ì´íŠ¸ ì•ˆ ë¨", "ì—°ê²° ì•ˆ ë¨", "ì‚­ì œë¨", unexpected on/off, error codes, etc.
- Examples:
  - "ì•±ì—ì„œ í•„í„°ë¥¼ ì¬ì„¤ì •í•˜ë ¤ê³  í•˜ë©´ 'ë„¤íŠ¸ì›Œí¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ' ì˜¤ë¥˜ê°€ ë‚©ë‹ˆë‹¤."
  - "ê±´ì¡°ê¸°ê°€ ì‹œì‘ë˜ì§€ ì•Šê³  E31 ì˜¤ë¥˜ ì½”ë“œê°€ í‘œì‹œë©ë‹ˆë‹¤."
  - "ì›Œì‹œíƒ€ì›Œ ê±´ì¡°ê¸° ì—…ê·¸ë ˆì´ë“œ ì™„ë£Œí–ˆëŠ”ë°ë„ ì—…ê°€ì „ì„¼í„°ì—ì„œëŠ” ê³„ì† ì—…ê·¸ë ˆì´ë“œë¡œ ëœ¹ë‹ˆë‹¤."
  - "ì—…ë°ì´íŠ¸í•  ë•Œë§ˆë‹¤ ì—°ê²°ì´ ì•ˆ ë©ë‹ˆë‹¤. í”„ë¡œê·¸ë¨ ì‹¤ë ¥ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê²Œ í•´ì£¼ì„¸ìš”."
  - "ì˜¤ë¸ ë¶ˆì´ ì €ì ˆë¡œ ì¼œì§‘ë‹ˆë‹¤."

[Complaints (ë¶ˆë§Œ/ë¶ˆí¸/ì‹¤ë§/ì§œì¦)]
- Signals: ì§œì¦/ë¶ˆí¸/ì‹¤ë§/ì–´ì²˜êµ¬ë‹ˆì—†ìŒ ë“± ê°ì • ì¤‘ì‹¬ì˜ ë¶€ì •ì  í‘œí˜„
- Examples:
  - "ì“¸ë°ì—†ëŠ” ì•Œë¦¼ ë³´ë‚´ì§€ ë§ˆì„¸ìš”. ë‹¤ì‹œëŠ” ì´ ë¸Œëœë“œ ì•ˆ ì”ë‹ˆë‹¤."
  - "ì‹œì‘/ì¢…ë£Œ ë²„íŠ¼ì„ ì§ì ‘ ëˆŒëŸ¬ì•¼ í•œë‹¤ë‹ˆ IoTê°€ ì–´ì²˜êµ¬ë‹ˆì—†ë„¤ìš”."
  - "ìŒì„±ì¸ì‹ ì¢€ ê°œì„ í•˜ì„¸ìš”. Gì‚¬ ìŠ¤í”¼ì»¤ëŠ” ì•„ì´ ë§ë„ ì˜ ì¸ì‹ë¼ìš”."
  - "ì´ëŸ° ì œí’ˆì— 2000ë‹¬ëŸ¬ë¥¼ ì“°ê²Œ í•˜ë‚˜ìš”?"
  - "ê±´ì¡° ì„±ëŠ¥ì´ í˜•í¸ì—†ìŠµë‹ˆë‹¤."

[Comments (ë¬¸ì˜/ì¤‘ë¦½ì  ê±´ì˜/ì§ˆë¬¸)]
- Signals: ê°ì •/ê²°í•¨ ì—†ì´ ì •ë³´/ì§ˆë¬¸/ì„¤ëª… ì¤‘ì‹¬
- Examples:
  - "ê´€ì‹¬ ì—†ëŠ” ì—…ë°ì´íŠ¸ëŠ” ìˆ¨ê¸¸ ìˆ˜ ìˆê²Œ í•´ì£¼ì„¸ìš”. ì„¸íƒê¸° ì•Œë¦¼ìŒ ë‹¤ì–‘í•˜ê²Œ í•„ìš” ì—†ì–´ìš”."
  - "ì‚¬ì „ì„¸íƒ ì‚¬ì´í´ì„ ì–´ë–»ê²Œ ì¼œë‚˜ìš”? ì˜µì…˜ì´ êº¼ì ¸ ìˆê³  ì¼œì§ì´ ì•ˆ ë©ë‹ˆë‹¤."
  - "ë¡œë´‡ ì²­ì†Œì‹œê°„ì— í™ˆë·°/ë¦¬ëª¨ì»¨ìœ¼ë¡œ ì²­ì†Œêµ¬ì—­ ì„¤ì •í•  ìˆ˜ ìˆë‚˜ìš”?"
  - "ë‚´ ì„¸íƒê¸°ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ë¥¼ ë°›ì•„ë³¼ ìˆ˜ ìˆì„ê¹Œìš”?"

<Output>
Return ONLY one word:
- "Suggestion" OR "ICC"
No extra text.

Input Summary:
{original_input}
""")

# (2) í†µí•© ì œì•ˆ ë¶„ë¦¬ ë° ìš”ì•½ (ì˜ì–´) â€” JSON ë°°ì—´ë¡œ ìœ ë„ (ì½”ë“œëŠ” íŒŒì‹±í•˜ì§€ ì•Šì•„ë„ OK)
combined_suggestion_prompt = PromptTemplate.from_template("""
<Role> You are an expert in understanding and extracting user suggestions from product feedback, specializing in home appliances. </Role>

<Task> Your task is to perform the following steps:
1.  Carefully read the provided user feedback, which might contain multiple distinct suggestions.
2.  **Separate each distinct suggestion clearly.** Identify and number each unique suggestion.
3.  For each identified suggestion, summarize it comprehensively while maintaining its distinctiveness and context, especially with respect to the component `{components}`.
4.  **Ensure all summarized suggestions are output in English.** Do not merge different suggestions into one if they are distinct.
5. The summarized sentences would be started by 'The user suggested'.

Reasoning: First, identify if the input contains more than one distinct idea (look for keywords like "and", punctuation, or multiple sentences indicating separate ideas). Then, for each distinct idea, summarize it comprehensively in English, ensuring it stands as a standalone improvement point related to `{components}`.

Text:
{original_input}

<Format>: Strictly follow these output rules:
- Output must be a valid JSON array of strings.
- The output must start with `[` and end with `]`, and contain no other text before or after.
- Do not include any explanations, labels, metadata, or extra text.
- Each string in the array should be a distinct, summarized suggestion in **English**.
</Format>
""")

# (3) ì²« ë²ˆì§¸ ì œì•ˆ ì„ íƒ (ì›ë¬¸ ìœ ì§€)
first_prompt = PromptTemplate.from_template("""<Role> You select the first suggestion from the comprehensive summary based on order of appearance. </Role>
<Task>: Identify and return the first suggestion from the comprehensive summary, exactly as it appears, without any additional text.

Reasoning: Use a Thought step to parse the comprehensive summary and find the first suggestion mentioned. Then Act by outputting that first suggestion verbatim. Do not include any formatting, numbering, or other suggestions.

Comprehensive Summary: 
{overall_summary}  

List of Original Suggestions:
{proposals}

<Format>: Output the first Suggestion as a plain text string (no JSON, no list formatting, no quotes around it).""")

sc_prompt = PromptTemplate.from_template("""
<Role> You are an expert who independently evaluates the most similar document based on the Suggestion statement and top 10 similarity documents. This process will be repeated 3 times to ensure Self-Consistency. </Role>

<Task>: Based on the following information, select the most similar document. Evaluate each criterion below with numeric scores (integers), and calculate the total evaluation score and thinking confidence at the end.

[Input Information]
Suggestion:
{proposal_summary}

Top 10 Documents:
{top_10_table}

<Evaluation Criteria>: Evaluate each criterion with quantitative scores.

- Selected Document (Ticket_id): [e.g., fdff64d]
- Functional Category Alignment (Context Entity Recall): [0-25 points]
- Claim Coverage (Context Recall): [0-25 points]
- Evidence Faithfulness: [0-25 points]
- Explanation Flow Similarity: [0-25 points]

<Qualitative Assessment Criteria>: Self-evaluate your reasoning process.

- Clarity of Criteria Application: [0-30 points]
- Logical Coherence: [0-30 points]
- Persuasiveness of Explanation: [0-20 points]
- Absence of Ambiguity: [0-20 points]

<Output JSON Schema>
Return ONLY a valid JSON object in the following format:
{{
  "top2": [
    {{
      "selected_ticket_id": "string",
      "scores": {{
        "func_align": int,
        "claim_coverage": int,
        "faithfulness": int,
        "flow_similarity": int,
        "total": int
      }},
      "self_eval": {{
        "clarity": int,
        "logic": int,
        "persuasion": int,
        "no_ambiguity": int,
        "confidence_pct": int
      }},
      "reason": "string"
    }},
    {{
      "selected_ticket_id": "string",
      "scores": {{
        "func_align": int,
        "claim_coverage": int,
        "faithfulness": int,
        "flow_similarity": int,
        "total": int
      }},
      "self_eval": {{
        "clarity": int,
        "logic": int,
        "persuasion": int,
        "no_ambiguity": int,
        "confidence_pct": int
      }},
      "reason": "string"
    }}
  ]
}}

<Rules>
- Output ONLY the JSON object (no markdown, no extra text).
- The two "selected_ticket_id" MUST be from the provided Top 10.
- Order the two items by your preference (best first).
""")

choose_prompt = PromptTemplate.from_template("""
<Role7> You are an expert who integrates three repeated evaluation results and determines the most reliable final recommended document. </Role7>

<Task>: Analyze the three Self-Consistency-based evaluation responses together with RAG-based similarity scores and RAGAS-based quantitative metrics to ultimately select the most similar document and explain your reasoning.

Reasoning: Collect all the following metrics to calculate an integrated score, then select the most comprehensively similar document.

<Integrated Evaluation Criteria>
1. Self-Consistency Average Confidence (Weight 0.3)
2. Same Document Repeated Selection (+10 point bonus)
3. Self-Consistency Average Total Score (Weight 0.2)
4. RAG-based Cosine Similarity (Weight 0.2)
5. Faithfulness (RAGAS metric) (Weight 0.15)
6. Context Recall (RAGAS metric) (Weight 0.15)

<Final Selection Criteria>
- Select the document with the highest integrated score.
- In case of equal scores, prioritize documents more frequently selected in Self-Consistency.
- The decision should be based on how similar the document summary, functional purpose, and explanation style are to the Suggestion statement.

Self-Consistency Response List:
{self_consistency_responses}

RAG Similarity List:
{top_10_table}

<Output JSON Schema>
Return ONLY a valid JSON object like:
{{
  "final_ticket_id": "string",
  "final_top2": [
    {{
      "ticket_id": "string",
      "rag_similarity_pct": number,
      "integrated_score": number,
      "reasons": [
        "string"
      ]
    }},
    {{
      "ticket_id": "string",
      "rag_similarity_pct": number,
      "integrated_score": number,
      "reasons": [
        "string"
      ]
    }}
  ],
  "notes": "string"
}}

<Rules>
- You MUST output exactly two items in "final_top2", ordered best-first.
- Strongly consider frequency across the three Self-Consistency runs (e.g., a ticket repeatedly selected should outrank others), then apply the integrated score.
- "final_ticket_id" must equal the first element's "ticket_id".
- Derive rag_similarity_pct from the given RAG table for each chosen ticket (0~1 â†’ Ã—100).
- Output ONLY the JSON object (no extra text).
""")



# In[7]: í—¬í¼ í•¨ìˆ˜ë“¤ -------------------------------------------------------------
def retrieve_context_native(query: str, component_group: str, collection, embed_model, ticket_to_keyword: dict = None):
    """ChromaDB ë„¤ì´í‹°ë¸Œ ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
       component_group/keyword/ìœ ì‚¬ë„ë¥¼ í¬í•¨í•œ í…Œì´ë¸” & ë ˆì½”ë“œ & ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜."""
    if ticket_to_keyword is None:
        ticket_to_keyword = TICKET_TO_KEYWORD

    # 1) ì¿¼ë¦¬ ì„ë² ë”©
    if hasattr(embed_model, "embed_query"):
        query_embedding = embed_model.embed_query(query)
    else:
        query_embedding = embed_model.encode(query).tolist()

    # 2) where í•„í„°
    where_filter = {"component_group": component_group} if component_group else {}

    # 3) ì»¬ë ‰ì…˜ ì§ˆì˜
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=10,
        where=where_filter
    )

    if not results["ids"] or not results["ids"][0]:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", [], [], "[]"

    rows, docs, records = [], [], []
    for idx, (doc_id, dist, meta, content) in enumerate(zip(
        results["ids"][0],
        results["distances"][0],
        results["metadatas"][0],
        results["documents"][0]
    ), start=1):
        ticket_id = meta.get("ticket_id", "N/A")
        doc_cg = meta.get("component_group", "N/A")
        kw = ticket_to_keyword.get(ticket_id, "N/A")
        summary = str(content).strip().replace("\n", " ")[:120]
        similarity = 1 - float(dist)  # cosine distance â†’ similarity

        rows.append(f"| {idx} | {ticket_id} | {doc_cg} | {kw} | {similarity:.4f} | {summary} |")
        records.append({
            "rank": idx,
            "ticket_id": ticket_id,
            "component_group": doc_cg,
            "keyword": kw,
            "similarity": round(similarity, 4),  # 0~1
            "summary": summary
        })
        docs.append(Document(page_content=content, metadata=meta))

    table = "\n".join([
        "| ìˆœìœ„ | Ticket ID | Component Group | Keyword | Cosine ìœ ì‚¬ë„ | ë¬¸ì„œ ìš”ì•½ |",
        "| --- | --- | --- | --- | --- | --- |",
        *rows
    ])
    top10_json = json.dumps(records, ensure_ascii=False)
    return table, docs, records, top10_json


def update_icc_df(x):
    """ICCë¡œ íŒì •ëœ ê²½ìš° icc_dfì— ì¶”ê°€ ì €ì¥ (ì œì•ˆ(Suggestion)ì´ë©´ ë¯¸ìˆ˜í–‰)"""
    if x.get("icc_check") != "ICC":
        return x  # Suggestionì´ë©´ ì•„ë¬´ ê²ƒë„ ì•ˆ í•˜ê³  ë°˜í™˜
    new_row = {
        "ticket_id": x.get("ticket_id_hashed", ""),
        "components": x.get("components", ""),
        "before_change": x.get("beforechange", ""),
        "after_change": x.get("afterchange", ""),
        "ICC": "ICC"
    }
    global icc_df
    icc_df = pd.concat([icc_df, pd.DataFrame([new_row])], ignore_index=True)
    return x


token_usage_results = []
def invoke_llm_with_token_tracking(prompt_template, input_data, step_name):
    """LLM í˜¸ì¶œ + í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ â€” ì‘ë‹µì€ 'ì›ë¬¸ ê·¸ëŒ€ë¡œ ë¬¸ìì—´'ë¡œ ë°˜í™˜"""
    with get_openai_callback() as cb:
        resp = llm.invoke(prompt_template.format(**input_data))
    token_usage_results.append({
        "step": step_name,
        "total_tokens": cb.total_tokens,
        "prompt_tokens": cb.prompt_tokens,
        "completion_tokens": cb.completion_tokens,
    })
    # LangChain ë©”ì‹œì§€ ê°ì²´ë©´ .content, ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return getattr(resp, "content", resp)


def select_rag_top2(records):
    """
    records: retrieve_context_native()ê°€ ë°˜í™˜í•œ dict ë¦¬ìŠ¤íŠ¸
             [{ticket_id, component_group, keyword, similarity(0~1), summary}, ...]
    return:
      final_result_json (str), top1_ticket_id (str), top1_keyword (str), top1_score_pct (float)
    """
    if not records:
        final_obj = {"final_ticket_id": "", "final_top2": [], "notes": "No RAG results"}
        return json.dumps(final_obj, ensure_ascii=False), "", "", 0.0

    sorted_rec = sorted(records, key=lambda r: r.get("similarity", 0), reverse=True)[:2]
    top2 = []
    for r in sorted_rec:
        pct = round(float(r.get("similarity", 0)) * 100, 2)
        top2.append({
            "ticket_id": r.get("ticket_id", ""),
            "rag_similarity_pct": pct,
            # Choose ë‹¨ê³„ë¥¼ ìƒëµí•˜ë¯€ë¡œ integrated_scoreëŠ” ê°„ë‹¨íˆ rag%ë¡œ ë‘ 
            "integrated_score": pct,
            "reasons": ["Selected solely by RAG cosine similarity (highest first)."]
        })

    final_obj = {
        "final_ticket_id": top2[0]["ticket_id"] if top2 else "",
        "final_top2": top2,
        "notes": "Self-Consistency/Choose prompts skipped; RAG-only selection."
    }
    top1_id = sorted_rec[0].get("ticket_id", "") if sorted_rec else ""
    top1_kw = sorted_rec[0].get("keyword", "") if sorted_rec else ""
    top1_pct = round(float(sorted_rec[0].get("similarity", 0)) * 100, 2) if sorted_rec else 0.0
    return json.dumps(final_obj, ensure_ascii=False), top1_id, top1_kw, top1_pct


# In[8]: ì²´ì¸ ì •ì˜ -------------------------------------------------------------
def _icc_branch(x):
    if x["icc_check"] == "ICC":
        y = update_icc_df(x)
        print("ğŸ‘‰ íŒë³„ ê²°ê³¼ : ICC")
        y = y | {"__is_icc": True}
        return y
    else:
        print("ğŸ‘‰ íŒë³„ ê²°ê³¼ : Proposal")
        return x | {"__is_icc": False}

chain = RunnableSequence(
    # Step 0: ì…ë ¥ë°›ê¸°
    RunnableLambda(timed("ì…ë ¥ë°›ê¸°")(lambda x: x)),

    # Step 1: ICC ë¶„ë¥˜
    RunnableLambda(timed("ICC ë¶„ë¥˜")(lambda x: x | {
        "icc_check": invoke_llm_with_token_tracking(
            prompt_template=icc_prompt,
            input_data={"original_input": x["original_input"]},
            step_name="ICC ë¶„ë¥˜"
        ).strip(),
        "component_group": x["component_group"]
    })),

    # Step 2: ICC íŒë‹¨ ë° ë¶„ê¸° (í”Œë˜ê·¸ë§Œ ì„¸ì›€)
    RunnableLambda(timed("ICC ë¶„ê¸°ì²˜ë¦¬")(_icc_branch)),

    # Step 3: ì œì•ˆ ë¶„ë¦¬ ë° ì „ì²´ ì œì•ˆ ìš”ì•½ (ì˜ì–´)
    RunnableLambda(timed("í†µí•© ì œì•ˆ ë¶„ë¦¬ ë° ìš”ì•½ (ì˜ì–´)")(lambda x: x | {
        "proposal_summary_all": invoke_llm_with_token_tracking(
            prompt_template=combined_suggestion_prompt,
            input_data={"original_input": x["original_input"], "components": x["components"]},
            step_name="í†µí•© ì œì•ˆ ë¶„ë¦¬ ë° ìš”ì•½ (ì˜ì–´)"
        ),
        "component_group": x["component_group"]
    })),

    # Step 4: ì²« ë²ˆì§¸ ì œì•ˆ ì„ íƒ
    RunnableLambda(timed("ì²«ë²ˆì§¸ ì œì•ˆ ì„ íƒ")(lambda x: x | {
        "first_proposal": invoke_llm_with_token_tracking(
            prompt_template=first_prompt,
            input_data={
                "overall_summary": x["proposal_summary_all"],   # íŒŒì‹± ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬
                "proposals": x["proposal_summary_all"]          # íŒŒì‹± ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬
            },
            step_name="ì²«ë²ˆì§¸ ì œì•ˆ ì„ íƒ"
        ).strip(),
        "component_group": x["component_group"]
    })),

    # Step 5: Top-10 ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (component_group + keyword í¬í•¨, JSONë„ ìƒì„±)
    RunnableLambda(timed("Top-10 ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰")(lambda x: x | (lambda table, docs, records, top10_json: {
        "top_10_table": table,
        "top_10_docs": docs,
        "top_10_records": records,
        "top_10_json": top10_json,
        "component_group": x["component_group"]
    })(*retrieve_context_native(
        query=x["first_proposal"],
        component_group=x["component_group"],
        collection=native_collection,
        embed_model=aoai_embeddings,
        ticket_to_keyword=TICKET_TO_KEYWORD
    )))),

    # Step 6: âœ… RAG Top-2 ì§ì ‘ ì„ íƒ (SC/Choose ìƒëµ)
    RunnableLambda(timed("RAG Top-2 ì„ íƒ")(lambda x: (lambda final_json, top1_id, top1_kw, top1_pct: x | {
        "final_result": final_json,          # ì§ì ‘ ë§Œë“  JSON ë¬¸ìì—´
        "ticket_id": top1_id,                # ìƒìœ„ 1ê°œ ticket_id
        "sim_keyword": top1_kw,              # ìƒìœ„ 1ê°œ ë¬¸ì„œì˜ keyword
        "sim_score": top1_pct                # ìƒìœ„ 1ê°œ ìœ ì‚¬ë„(%) â†’ í›„ì† ë¡œì§ì˜ 90ì»·ê³¼ í˜¸í™˜
    })(*select_rag_top2(x["top_10_records"]))))
)


# In[9]: main() ----------------------------------------------------------------
def main(inputs):
    """
    inputs ì˜ˆì‹œ:
    {
        "original_input": "ThinkQ í‰ë©´ë„ìƒì— ì„ ì„ ê·¸ì–´ ì²­ì†Œêµ¬ì—­ì„ ì§€ì •í•˜ë„ë¡ í•´ì£¼ì„¸ìš”",
        "components": "ë¡œë´‡ì²­ì†Œê¸°",
        "component_group": "robot",
        "afterchange": "",
        "beforechange": "",
        "ticket_id_hashed": "xxxx"
    }
    """
    global timing_results, token_usage_results, icc_df
    timing_results = []
    token_usage_results = []

    print("\nğŸ“¥ ì…ë ¥ ì œì•ˆë¬¸:")
    print(inputs["original_input"])

    result = chain.invoke(inputs)
    print("âœ… result keys:", list(result.keys()))

    # ICCì¸ ê²½ìš°: ìµœì†Œ ì •ë³´ë¡œ ë°˜í™˜ (RAG/ìš”ì•½ ìŠ¤í‚µ)
    if result.get("__is_icc"):
        print("\nğŸ”” ICCë¡œ íŒì •ë˜ì–´ RAG Top-2 ì„ íƒì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        final_json = json.dumps(
            {"final_ticket_id": "", "final_top2": [], "notes": "ICC item"},
            ensure_ascii=False
        )
        return (
            inputs.get("ticket_id_hashed", ""),      # 1 ticket_id
            inputs.get("components", ""),            # 2 components
            inputs.get("beforechange", ""),          # 3 before_change
            inputs.get("afterchange", ""),           # 4 after_change
            "",                                      # 5 generated_summary
            0.0,                                     # 6 sim_score
            "",                                      # 7 sim_keyword
            timing_results,                          # 8 timing_results
            "",                                      # 9 proposal_summary_all
            inputs.get("component_group", ""),       # 10 component_group
            final_json,                              # 11 final_result (JSON)
            token_usage_results,                     # 12 token_usage_results
            icc_df.copy(),                           # 13 icc_df
            "",                                      # 14 top_10_table
            []                                       # 15 top_10_records
        )

    # ì¼ë°˜(Suggestion) íë¦„: ìš”ì•½/Top-10/RAG Top-2 ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“„ ì „ì²´ ì œì•ˆ ìš”ì•½(LLM ì›ë¬¸ ê·¸ëŒ€ë¡œ):")
    print(result["proposal_summary_all"])

    print("\nâ­ ì²« ë²ˆì§¸ ì œì•ˆ(LLM ì›ë¬¸ ê·¸ëŒ€ë¡œ):")
    print(result["first_proposal"])

    print("\nğŸ” Top-10 ìœ ì‚¬ ë¬¸ì„œ í…Œì´ë¸”:")
    print(result["top_10_table"])

    print("\nğŸ ìµœì¢… ê²°ì •(RAG Top-2 JSON):")
    print(result["final_result"])

    # --- í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ---
    print("\n--- ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ---")
    total_prompt_tokens = sum(t.get("prompt_tokens", 0) for t in token_usage_results)
    total_completion_tokens = sum(t.get("completion_tokens", 0) for t in token_usage_results)
    total_tokens_overall = sum(t.get("total_tokens", 0) for t in token_usage_results)

    print(f"ì´ í”„ë¡¬í”„íŠ¸ í† í°: {total_prompt_tokens}")
    print(f"ì´ ì™„ì„± í† í°: {total_completion_tokens}")
    print(f"ì „ì²´ í† í° ì‚¬ìš©ëŸ‰: {total_tokens_overall}")
    print("ë‹¨ê³„ë³„ í† í° ì‚¬ìš©ëŸ‰:")
    for entry in token_usage_results:
        print(f"- {entry['step']}: ì´ {entry['total_tokens']} (í”„ë¡¬í”„íŠ¸: {entry['prompt_tokens']}, ì™„ì„±: {entry['completion_tokens']})")

    # --- ë°˜í™˜ (ì •í™•íˆ 15ê°œ) ---
    ticket_id = result.get("ticket_id", "")
    components = inputs.get("components", "")
    before_change = inputs.get("beforechange", "")
    after_change = inputs.get("afterchange", "")
    generated_summary = result.get("first_proposal", inputs.get("generated_summary", ""))
    sim_score = float(result.get("sim_score", 0.0))        # Top-1 ìœ ì‚¬ë„(%)
    sim_keyword = result.get("sim_keyword", "")            # Top-1 keyword
    proposal_summary_all = result.get("proposal_summary_all", "")
    component_group = result.get("component_group", inputs.get("component_group", ""))
    top_10_table = result.get("top_10_table", "")
    top_10_records = result.get("top_10_records", [])
    final_result = result.get("final_result", "")          # RAG Top-2 JSON ë¬¸ìì—´

    return (
        ticket_id,                    # 1
        components,                   # 2
        before_change,                # 3
        after_change,                 # 4
        generated_summary,            # 5
        sim_score,                    # 6  (Top-1 % â†’ ê¸°ì¡´ 90 ì»·ê³¼ í˜¸í™˜)
        sim_keyword,                  # 7  (Top-1ì˜ keyword)
        timing_results,               # 8
        proposal_summary_all,         # 9
        component_group,              # 10
        final_result,                 # 11 (RAG Top-2 JSON)
        token_usage_results,          # 12
        icc_df.copy(),                # 13
        top_10_table,                 # 14
        top_10_records                # 15
    )

if __name__ == "__main__":
    main()

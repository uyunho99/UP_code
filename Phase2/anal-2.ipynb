{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5fed1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Input file paths\n",
    "nsc_files = [\n",
    "    \"./result_dup_nsc.csv\",\n",
    "    \"./result_new_nsc.csv\",\n",
    "]\n",
    "\n",
    "non_nsc_files = [\n",
    "    \"./result_dup.csv\",\n",
    "    \"./result_new.csv\",\n",
    "]\n",
    "\n",
    "# Output file paths\n",
    "out_nsc = Path(\"./concat_nsc.csv\")\n",
    "out_non_nsc = Path(\"./concat_non_nsc.csv\")\n",
    "\n",
    "def load_csv_safe(path):\n",
    "    # Use utf-8-sig to be consistent with user's previous files\n",
    "    return pd.read_csv(path, encoding=\"utf-8-sig\", low_memory=False)\n",
    "\n",
    "# Load inputs\n",
    "dfs_nsc = [load_csv_safe(p) for p in nsc_files]\n",
    "dfs_non_nsc = [load_csv_safe(p) for p in non_nsc_files]\n",
    "\n",
    "# Concatenate\n",
    "concat_nsc = pd.concat(dfs_nsc, ignore_index=True, sort=False)\n",
    "concat_non_nsc = pd.concat(dfs_non_nsc, ignore_index=True, sort=False)\n",
    "\n",
    "# Save results\n",
    "concat_nsc.to_csv(out_nsc, index=False, encoding=\"utf-8-sig\")\n",
    "concat_non_nsc.to_csv(out_non_nsc, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed9701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] origin columns: ['ticket_id_hashed', 'Privacy_Detect_Col', 'components', 'keyword', '사업부', '지역', 'language', '대분류', '중분류', '소분류', 'beforechange', 'afterchange', 'labels', 'environment', 'created_date_yyyymm', 'thinQmodel', 'salesmodel', 'generated_summary', 'generated_translation', 'merge_key', 'generated_response']\n",
      "[Select:keyword] exact -> 'keyword'\n",
      "[Join] 'ticket_id_hashed'로 Keyword_from_origin 부착 → 200/200 rows matched\n",
      "[Warn] 'final_top2'도 'final_result_raw'도 없어 sim_keyword_* 생성 불가\n",
      "[Check] 키워드 관련 컬럼: ['sim_keyword', 'final_keyword', 'Keyword_from_origin']\n",
      "\n",
      "[Keyword NaN 통계]\n",
      "                      null  null_rate(%)\n",
      "sim_keyword            10           5.0\n",
      "final_keyword           0           0.0\n",
      "Keyword_from_origin     0           0.0\n",
      "\n",
      "[Rows with any keyword NaN] 10 / 200 (5.00%)\n",
      "\n",
      "[미싱 샘플 상위 10건]\n",
      "ticket_id_hashed sim_keyword final_keyword Keyword_from_origin\n",
      "         2597c3b         NaN        세탁건조통합              원격기능제어\n",
      "         565d6e1         NaN          건조효율               계절별테마\n",
      "         427edf9         NaN          세척효율       올인원세제린스부족알림끄기\n",
      "         b5049ab         NaN         세탁물관리        내부조명on/off설정\n",
      "         dc66479         NaN          건조효율                털기모드\n",
      "         fb86292         NaN       냉장고온도조절          아이스메이커원격설정\n",
      "         cb33aea         NaN       냉장고온도조절              필터상태표시\n",
      "         6aa6a86         NaN         세탁물관리               앱알림설정\n",
      "         bf49642         NaN          정수필터      디스플레이밝기조절설정/변경\n",
      "         3a03509         NaN         세탁물관리               케어서비스\n",
      "\n",
      "[Saved audit] ./keyword_nan_audit.csv\n",
      "Saved -> ./concat_nsc_plus_keyword_with_top2_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# 0) 파일 경로\n",
    "# =========================\n",
    "non_nsc_path = \"./concat_non_nsc.csv\"\n",
    "origin_path  = \"./all_origin_updated.csv\"\n",
    "\n",
    "# =========================\n",
    "# 1) 유틸: 컬럼 선택기\n",
    "# =========================\n",
    "def pick_column(df, exact_priority=None, contains_priority=None, reason=\"\"):\n",
    "    \"\"\"\n",
    "    df.columns 중에서 우선순위에 맞춰 컬럼 하나를 고른다.\n",
    "    - exact_priority: 소문자 동일비교 우선순위 리스트\n",
    "    - contains_priority: '포함' 우선순위 리스트(소문자)\n",
    "    못 찾으면 None\n",
    "    \"\"\"\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    # exact match\n",
    "    if exact_priority:\n",
    "        for cand in exact_priority:\n",
    "            if cand in cols_lower:\n",
    "                print(f\"[Select:{reason}] exact -> '{cols_lower[cand]}'\")\n",
    "                return cols_lower[cand]\n",
    "    # contains match\n",
    "    if contains_priority:\n",
    "        for partial in contains_priority:\n",
    "            for c in df.columns:\n",
    "                if partial in c.lower():\n",
    "                    print(f\"[Select:{reason}] contains('{partial}') -> '{c}'\")\n",
    "                    return c\n",
    "    return None\n",
    "\n",
    "def norm_str(x):\n",
    "    return None if pd.isna(x) else str(x).strip()\n",
    "\n",
    "# =========================\n",
    "# 2) 로드 & 기본 정리\n",
    "# =========================\n",
    "non_nsc = pd.read_csv(non_nsc_path, encoding=\"utf-8-sig\", low_memory=False)\n",
    "origin  = pd.read_csv(origin_path,  encoding=\"utf-8-sig\", low_memory=False)\n",
    "\n",
    "for df in (non_nsc, origin):\n",
    "    if \"ticket_id_hashed\" in df.columns:\n",
    "        df[\"ticket_id_hashed\"] = df[\"ticket_id_hashed\"].astype(str).str.strip()\n",
    "\n",
    "print(\"[Info] origin columns:\", list(origin.columns))\n",
    "\n",
    "# =========================\n",
    "# 3) origin의 '키워드' 컬럼 자동 탐지\n",
    "#    - 우선순위: exact > contains('keyword') > contains('answer')\n",
    "# =========================\n",
    "keyword_exact_priority    = [\n",
    "    \"keyword\", \"keywords\", \"keyword_from_origin\", \"answer_keyword_main\",\n",
    "    \"answer_keyword\", \"final_keyword\"\n",
    "]\n",
    "keyword_contains_priority = [\"keyword\", \"answer\"]\n",
    "\n",
    "ORIGIN_KEYWORD_COL = pick_column(\n",
    "    origin,\n",
    "    exact_priority=keyword_exact_priority,\n",
    "    contains_priority=keyword_contains_priority,\n",
    "    reason=\"keyword\"\n",
    ")\n",
    "\n",
    "if ORIGIN_KEYWORD_COL is None:\n",
    "    raise KeyError(\n",
    "        \"키워드 컬럼을 찾지 못했습니다. origin에 다음 중 하나의 컬럼이 필요합니다: \"\n",
    "        f\"exact={keyword_exact_priority}, contains any of {keyword_contains_priority}. \"\n",
    "        f\"현재 컬럼: {list(origin.columns)}\"\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# 4) JOIN 키 결정: 기본은 ticket_id_hashed\n",
    "#    - 없으면 ticket_id로 폴백 (non_nsc와 origin 둘 다 있어야 함)\n",
    "# =========================\n",
    "JOIN_KEY = None\n",
    "if \"ticket_id_hashed\" in non_nsc.columns and \"ticket_id_hashed\" in origin.columns:\n",
    "    JOIN_KEY = \"ticket_id_hashed\"\n",
    "elif \"ticket_id\" in non_nsc.columns and \"ticket_id\" in origin.columns:\n",
    "    JOIN_KEY = \"ticket_id\"\n",
    "else:\n",
    "    # 그래도 진행은 가능(나중에 final_top2 매핑은 dict로 처리)하지만\n",
    "    # 병합은 스킵하고 경고 출력\n",
    "    print(\"[Warn] 공통 JOIN 키가 없어 병합을 스킵합니다. \"\n",
    "          \"final_top2 기반 sim_keyword_* 생성만 진행합니다.\")\n",
    "\n",
    "# =========================\n",
    "# 5) non_nsc ← origin 키워드 붙이기(LEFT JOIN)\n",
    "# =========================\n",
    "merged = non_nsc.copy()\n",
    "\n",
    "if JOIN_KEY is not None:\n",
    "    origin_kw = (\n",
    "        origin.loc[:, [JOIN_KEY, ORIGIN_KEYWORD_COL]]\n",
    "              .dropna(subset=[JOIN_KEY])\n",
    "              .drop_duplicates(subset=[JOIN_KEY], keep=\"first\")\n",
    "              .rename(columns={ORIGIN_KEYWORD_COL: \"Keyword_from_origin\"})\n",
    "    )\n",
    "    merged = merged.merge(origin_kw, on=JOIN_KEY, how=\"left\")\n",
    "    matched = merged[\"Keyword_from_origin\"].notna().sum()\n",
    "    print(f\"[Join] '{JOIN_KEY}'로 Keyword_from_origin 부착 → {matched}/{len(merged)} rows matched\")\n",
    "else:\n",
    "    print(\"[Join] 스킵(공통 키 없음)\")\n",
    "\n",
    "# =========================\n",
    "# 6) origin 키워드 조회 딕셔너리(해시/플레인 모두)\n",
    "# =========================\n",
    "kw_by_hashed = {}\n",
    "if \"ticket_id_hashed\" in origin.columns:\n",
    "    tmp = origin[[c for c in [\"ticket_id_hashed\", ORIGIN_KEYWORD_COL] if c in origin.columns]]\n",
    "    tmp = tmp.dropna(subset=[\"ticket_id_hashed\"])\n",
    "    for k, v in zip(tmp[\"ticket_id_hashed\"].map(norm_str), tmp[ORIGIN_KEYWORD_COL]):\n",
    "        if k and k not in kw_by_hashed:\n",
    "            kw_by_hashed[k] = v\n",
    "\n",
    "kw_by_plain = {}\n",
    "if \"ticket_id\" in origin.columns:\n",
    "    tmp = origin[[c for c in [\"ticket_id\", ORIGIN_KEYWORD_COL] if c in origin.columns]]\n",
    "    tmp = tmp.dropna(subset=[\"ticket_id\"])\n",
    "    for k, v in zip(tmp[\"ticket_id\"].map(norm_str), tmp[ORIGIN_KEYWORD_COL]):\n",
    "        if k and k not in kw_by_plain:\n",
    "            kw_by_plain[k] = v\n",
    "\n",
    "# =========================\n",
    "# 7) final_top2 파서 & id→keyword 매핑\n",
    "# =========================\n",
    "def parse_final_top2(value):\n",
    "    \"\"\"value에서 [id1, id2]를 뽑아낸다.\"\"\"\n",
    "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
    "        return [None, None]\n",
    "\n",
    "    obj = value\n",
    "    if isinstance(value, str):\n",
    "        s = value.strip()\n",
    "        try:\n",
    "            obj = json.loads(s)\n",
    "        except Exception:\n",
    "            obj = s\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        items = obj\n",
    "    elif isinstance(obj, dict):\n",
    "        # 흔한 키들 탐색\n",
    "        for k in (\"final_top2\", \"top2\", \"final\", \"result\"):\n",
    "            if k in obj:\n",
    "                sub = obj[k]\n",
    "                if isinstance(sub, dict):\n",
    "                    if \"top2\" in sub:\n",
    "                        items = sub[\"top2\"]\n",
    "                    elif \"final_top2\" in sub:\n",
    "                        items = sub[\"final_top2\"]\n",
    "                    else:\n",
    "                        items = sub\n",
    "                else:\n",
    "                    items = sub\n",
    "                break\n",
    "        else:\n",
    "            items = obj\n",
    "    else:\n",
    "        tokens = re.findall(r\"[A-Za-z0-9]{6,}\", str(obj))\n",
    "        ids = tokens[:2]\n",
    "        while len(ids) < 2:\n",
    "            ids.append(None)\n",
    "        return ids\n",
    "\n",
    "    ids = []\n",
    "    if isinstance(items, list):\n",
    "        for it in items:\n",
    "            if isinstance(it, dict):\n",
    "                for key in (\"ticket_id\", \"ticket_id_hashed\", \"id\"):\n",
    "                    if key in it and it[key] is not None:\n",
    "                        ids.append(norm_str(it[key]))\n",
    "                        break\n",
    "            else:\n",
    "                ids.append(norm_str(it))\n",
    "            if len(ids) >= 2:\n",
    "                break\n",
    "    elif isinstance(items, dict):\n",
    "        for key in (\"ticket_id\", \"ticket_id_hashed\", \"id\"):\n",
    "            if key in items and items[key] is not None:\n",
    "                ids.append(norm_str(items[key]))\n",
    "        if len(ids) < 2:\n",
    "            for v in items.values():\n",
    "                if isinstance(v, list):\n",
    "                    for it in v:\n",
    "                        if isinstance(it, dict):\n",
    "                            for key in (\"ticket_id\", \"ticket_id_hashed\", \"id\"):\n",
    "                                if key in it and it[key] is not None:\n",
    "                                    ids.append(norm_str(it[key]))\n",
    "                                    break\n",
    "                        else:\n",
    "                            ids.append(norm_str(it))\n",
    "                        if len(ids) >= 2:\n",
    "                            break\n",
    "                if len(ids) >= 2:\n",
    "                    break\n",
    "    else:\n",
    "        tokens = re.findall(r\"[A-Za-z0-9]{6,}\", str(items))\n",
    "        ids = tokens[:2]\n",
    "\n",
    "    while len(ids) < 2:\n",
    "        ids.append(None)\n",
    "    return ids[:2]\n",
    "\n",
    "def id_to_keyword(tid):\n",
    "    if not tid:\n",
    "        return None\n",
    "    # hashed 우선\n",
    "    if tid in kw_by_hashed:\n",
    "        return kw_by_hashed[tid]\n",
    "    # plain 보조\n",
    "    if tid in kw_by_plain:\n",
    "        return kw_by_plain[tid]\n",
    "    return None\n",
    "\n",
    "def top2_to_keywords(val):\n",
    "    id1, id2 = parse_final_top2(val)\n",
    "    return pd.Series([id_to_keyword(id1), id_to_keyword(id2)], index=[\"sim_keyword_1\", \"sim_keyword_2\"])\n",
    "\n",
    "# =========================\n",
    "# 8) sim_keyword_1/2 생성\n",
    "#    - 우선순위: 'final_top2' 컬럼 > 'final_result_raw' 컬럼\n",
    "# =========================\n",
    "if \"final_top2\" in merged.columns:\n",
    "    merged[[\"sim_keyword_1\", \"sim_keyword_2\"]] = merged[\"final_top2\"].apply(top2_to_keywords)\n",
    "elif \"final_result_raw\" in merged.columns:\n",
    "    merged[[\"sim_keyword_1\", \"sim_keyword_2\"]] = merged[\"final_result_raw\"].apply(top2_to_keywords)\n",
    "else:\n",
    "    print(\"[Warn] 'final_top2'도 'final_result_raw'도 없어 sim_keyword_* 생성 불가\")\n",
    "\n",
    "# =========================\n",
    "# 9) 키워드 NaN 감사(중간 출력 + CSV 저장)\n",
    "# =========================\n",
    "keyword_cols = [c for c in merged.columns if re.search(r'keyword', c, flags=re.I)]\n",
    "if not keyword_cols:\n",
    "    print(\"[Info] 'keyword' 포함 컬럼 없음\")\n",
    "else:\n",
    "    print(f\"[Check] 키워드 관련 컬럼: {keyword_cols}\")\n",
    "\n",
    "    stats = pd.DataFrame({\n",
    "        \"non_null\": merged[keyword_cols].notna().sum(),\n",
    "        \"null\": merged[keyword_cols].isna().sum()\n",
    "    })\n",
    "    stats[\"null_rate(%)\"] = (stats[\"null\"] / len(merged) * 100).round(2)\n",
    "    print(\"\\n[Keyword NaN 통계]\\n\", stats)\n",
    "\n",
    "    merged[\"is_missing_any_keyword\"] = merged[keyword_cols].isna().any(axis=1)\n",
    "\n",
    "    def _missing_cols(row):\n",
    "        miss = [col for col in keyword_cols if pd.isna(row[col])]\n",
    "        return \"|\".join(miss) if miss else None\n",
    "\n",
    "    merged[\"missing_keyword_cols\"] = merged.apply(_missing_cols, axis=1)\n",
    "\n",
    "    any_missing = merged[\"is_missing_any_keyword\"].sum()\n",
    "    print(f\"\\n[Rows with any keyword NaN] {any_missing} / {len(merged)} \"\n",
    "          f\"({any_missing/len(merged):.2%})\")\n",
    "\n",
    "    if any_missing:\n",
    "        cols_to_show = [c for c in [\"ticket_id_hashed\", \"ticket_id\"] if c in merged.columns] + keyword_cols\n",
    "        print(\"\\n[미싱 샘플 상위 10건]\")\n",
    "        print(merged.loc[merged[\"is_missing_any_keyword\"], cols_to_show]\n",
    "                    .head(10)\n",
    "                    .to_string(index=False))\n",
    "\n",
    "    audit_path = \"./keyword_nan_audit.csv\"\n",
    "    merged.loc[merged[\"is_missing_any_keyword\"],\n",
    "               [c for c in [\"ticket_id_hashed\", \"ticket_id\", \"missing_keyword_cols\"] if c in merged.columns]\n",
    "               + keyword_cols] \\\n",
    "          .to_csv(audit_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n[Saved audit] {audit_path}\")\n",
    "\n",
    "# =========================\n",
    "# 10) 최종 저장\n",
    "# =========================\n",
    "out_path2 = \"./concat_non_nsc_plus_keyword_with_top2_keywords.csv\"\n",
    "merged.to_csv(out_path2, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Saved -> {out_path2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Keyword NaN 통계]\n",
      "                      non_null  null  null_rate(%)\n",
      "sim_keyword               190    10           5.0\n",
      "final_keyword             200     0           0.0\n",
      "Keyword_from_origin       200     0           0.0\n",
      "sim_keyword_1               0   200         100.0\n",
      "sim_keyword_2               0   200         100.0\n",
      "Saved -> ./concat_nsc_plus_keyword_with_top2_keywords.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k4/dp8jpl_52w1g8nsm7nqcr12m0000gn/T/ipykernel_3439/996474099.py:202: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  ).applymap(id_to_keyword)\n"
     ]
    }
   ],
   "source": [
    "# Fix: extract ticket_ids from final_result.final_top2 and map to keywords from all_origin_updated\n",
    "import pandas as pd\n",
    "import json, re\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "nsc_path = \"./concat_nsc.csv\"\n",
    "origin_path = \"./all_origin_updated.csv\"\n",
    "\n",
    "nsc = pd.read_csv(nsc_path, encoding=\"utf-8-sig\", low_memory=False)\n",
    "origin = pd.read_csv(origin_path, encoding=\"utf-8-sig\", low_memory=False)\n",
    "\n",
    "def norm(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    return str(s).strip()\n",
    "\n",
    "# --- Build keyword lookup (hashed + plain) ---\n",
    "kw_col = None\n",
    "for c in origin.columns:\n",
    "    if c.lower() == \"keyword\":\n",
    "        kw_col = c; break\n",
    "if kw_col is None:\n",
    "    for c in origin.columns:\n",
    "        if \"keyword\" in c.lower():\n",
    "            kw_col = c; break\n",
    "if kw_col is None:\n",
    "    raise KeyError(f\"Keyword column not found in origin: {list(origin.columns)}\")\n",
    "\n",
    "kw_by_hashed = {}\n",
    "if \"ticket_id_hashed\" in origin.columns:\n",
    "    tmp = origin[[\"ticket_id_hashed\", kw_col]].dropna(subset=[\"ticket_id_hashed\"])\n",
    "    for k, v in zip(tmp[\"ticket_id_hashed\"].map(norm), tmp[kw_col]):\n",
    "        if k and k not in kw_by_hashed:\n",
    "            kw_by_hashed[k] = v\n",
    "\n",
    "kw_by_plain = {}\n",
    "if \"ticket_id\" in origin.columns:\n",
    "    tmp = origin[[\"ticket_id\", kw_col]].dropna(subset=[\"ticket_id\"])\n",
    "    for k, v in zip(tmp[\"ticket_id\"].map(norm), tmp[kw_col]):\n",
    "        if k and k not in kw_by_plain:\n",
    "            kw_by_plain[k] = v\n",
    "\n",
    "def id_to_keyword(tid):\n",
    "    if not tid:\n",
    "        return None\n",
    "    # hashed first\n",
    "    if tid in kw_by_hashed:\n",
    "        return kw_by_hashed[tid]\n",
    "    if tid in kw_by_plain:\n",
    "        return kw_by_plain[tid]\n",
    "    return None\n",
    "\n",
    "# --- Robust extractor from final_result ---\n",
    "def safe_json_loads(val):\n",
    "    if val is None or (isinstance(val, float) and pd.isna(val)):\n",
    "        return None\n",
    "    if isinstance(val, (dict, list)):\n",
    "        return val\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def extract_first_token(value_str):\n",
    "    if value_str is None:\n",
    "        return None\n",
    "    toks = re.findall(r\"[A-Za-z0-9]{6,}\", str(value_str))\n",
    "    return toks[0] if toks else None\n",
    "\n",
    "def extract_id_from_item(it):\n",
    "    if it is None:\n",
    "        return None\n",
    "    if isinstance(it, dict):\n",
    "        # keys that might contain the id\n",
    "        for k in (\"ticket_id_hashed\", \"ticket_id\", \"id\", \"doc_id\", \"row_id\", \"source_id\", \"hash\", \"uuid\"):\n",
    "            if k in it and it[k] is not None:\n",
    "                return extract_first_token(it[k])\n",
    "        # nested metadata-like\n",
    "        for k in (\"metadata\", \"source\", \"context\"):\n",
    "            if k in it and it[k] is not None:\n",
    "                v = it[k]\n",
    "                if isinstance(v, dict):\n",
    "                    for kk in (\"ticket_id_hashed\", \"ticket_id\", \"id\"):\n",
    "                        if kk in v and v[kk] is not None:\n",
    "                            return extract_first_token(v[kk])\n",
    "                else:\n",
    "                    return extract_first_token(v)\n",
    "        # fallback: any 6+ alnum token inside dict\n",
    "        return extract_first_token(json.dumps(it, ensure_ascii=False))\n",
    "    else:\n",
    "        # string-like item\n",
    "        return extract_first_token(it)\n",
    "\n",
    "def parse_final_top2_from_final_result(val):\n",
    "    obj = safe_json_loads(val)\n",
    "    if obj is None:\n",
    "        return [None, None]\n",
    "\n",
    "    items = None\n",
    "    if isinstance(obj, dict):\n",
    "        # common containers, especially \"final_top2\"\n",
    "        for key in (\"final_top2\", \"top2\", \"candidates\", \"items\", \"docs\", \"matches\", \"result\"):\n",
    "            if key in obj and isinstance(obj[key], list) and len(obj[key]) > 0:\n",
    "                items = obj[key]\n",
    "                break\n",
    "        if items is None:\n",
    "            # sometimes \"final_result\" is dict with nested dicts/lists\n",
    "            for v in obj.values():\n",
    "                if isinstance(v, list) and len(v) > 0:\n",
    "                    items = v\n",
    "                    break\n",
    "            if items is None:\n",
    "                # fall back: scan tokens\n",
    "                token = extract_first_token(json.dumps(obj, ensure_ascii=False))\n",
    "                return [token, None]\n",
    "    elif isinstance(obj, list):\n",
    "        items = obj\n",
    "    else:\n",
    "        # string → tokens\n",
    "        token = extract_first_token(obj)\n",
    "        return [token, None]\n",
    "\n",
    "    # sort by score if available\n",
    "    def score_of(it):\n",
    "        if not isinstance(it, dict):\n",
    "            return None\n",
    "        for k in (\"score\", \"similarity\", \"cosine\", \"sim\", \"confidence\"):\n",
    "            if k in it:\n",
    "                try:\n",
    "                    return float(it[k])\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return None\n",
    "\n",
    "    ordered = items\n",
    "    if items and isinstance(items[0], dict):\n",
    "        with_sc, without_sc = [], []\n",
    "        for it in items:\n",
    "            sc = score_of(it)\n",
    "            (with_sc if sc is not None else without_sc).append((sc, it))\n",
    "        if with_sc:\n",
    "            with_sc.sort(key=lambda x: x[0], reverse=True)\n",
    "            ordered = [it for sc, it in with_sc] + [it for sc, it in without_sc]\n",
    "\n",
    "    ids = []\n",
    "    for it in ordered:\n",
    "        ids.append(extract_id_from_item(it))\n",
    "        if len(ids) >= 2:\n",
    "            break\n",
    "    while len(ids) < 2:\n",
    "        ids.append(None)\n",
    "    return ids[:2]\n",
    "\n",
    "# --- Apply to NSC df ---\n",
    "if \"final_result\" not in nsc.columns:\n",
    "    raise KeyError(\"final_result column not found in concat_nsc.csv\")\n",
    "\n",
    "ids_df = nsc[\"final_result\"].apply(parse_final_top2_from_final_result).apply(pd.Series)\n",
    "ids_df.columns = [\"top2_id_1\", \"top2_id_2\"]\n",
    "\n",
    "nsc_aug = pd.concat([nsc, ids_df], axis=1)\n",
    "nsc_aug[\"sim_keyword_1\"] = nsc_aug[\"top2_id_1\"].map(id_to_keyword)\n",
    "nsc_aug[\"sim_keyword_2\"] = nsc_aug[\"top2_id_2\"].map(id_to_keyword)\n",
    "\n",
    "# attach Keyword_from_origin via join key if present\n",
    "if \"ticket_id_hashed\" in nsc_aug.columns and \"ticket_id_hashed\" in origin.columns:\n",
    "    origin_simple = origin[[\"ticket_id_hashed\", kw_col]].drop_duplicates(\"ticket_id_hashed\").rename(columns={kw_col: \"Keyword_from_origin\"})\n",
    "    nsc_aug = nsc_aug.merge(origin_simple, on=\"ticket_id_hashed\", how=\"left\")\n",
    "\n",
    "# Save\n",
    "out_path = \"./concat_nsc_plus_keyword_with_top2_keywords_fixed.csv\"\n",
    "nsc_aug.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Summaries to help debug\n",
    "summary = {\n",
    "    \"rows\": len(nsc_aug),\n",
    "    \"non_null_sim_keyword_1\": int(nsc_aug[\"sim_keyword_1\"].notna().sum()),\n",
    "    \"non_null_sim_keyword_2\": int(nsc_aug[\"sim_keyword_2\"].notna().sum())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f828d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Threshold ≥  Rows  kw_vs_answer_exact (cnt)  kw_vs_answer_exact (%)  kw_vs_final_exact (cnt)  kw_vs_final_exact (%)  kw_vs_answer_cossim (avg)  kw_vs_final_cossim (avg)  sim_summary_gen_1 (avg)  sim_summary_gen_2 (avg)\n",
      "          90    10                         9                   90.00                        9                  90.00                      0.923                     0.923                    0.888                      1.0\n",
      "          85    32                        25                   78.12                       10                  31.25                      0.899                     0.711                    0.854                      1.0\n",
      "          80    62                        44                   70.97                       10                  16.13                      0.878                     0.650                    0.812                      1.0\n",
      "          75    82                        58                   70.73                       10                  12.20                      0.871                     0.641                    0.783                      1.0\n",
      "          70    92                        66                   71.74                       10                  10.87                      0.876                     0.634                    0.765                      1.0\n",
      "Saved: ./similarity_stats_mini_dup_1_by_threshold.csv\n"
     ]
    }
   ],
   "source": [
    "# Threshold-based match analysis for NSC & Non-NSC using sim_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import caas_jupyter_tools\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def norm(s):\n",
    "    if pd.isna(s): return None\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "def pick_col(df, target_lower, contains=False):\n",
    "    # exact match first\n",
    "    for c in df.columns:\n",
    "        if c.lower() == target_lower:\n",
    "            return c\n",
    "    if contains:\n",
    "        for c in df.columns:\n",
    "            if target_lower in c.lower():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def ensure_numeric(series):\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def compute_stats(df, thresholds_decimals):\n",
    "    # column picks\n",
    "    col_origin = pick_col(df, \"keyword_from_origin\") or pick_col(df, \"keyword_from_origin\", contains=True)\n",
    "    col_sim1   = pick_col(df, \"sim_keyword_1\") or pick_col(df, \"sim_keyword_1\", contains=True)\n",
    "    col_sim2   = pick_col(df, \"sim_keyword_2\") or pick_col(df, \"sim_keyword_2\", contains=True)\n",
    "    col_simscore = pick_col(df, \"sim_score\") or pick_col(df, \"simscore\", contains=True) or pick_col(df, \"similarity\", contains=True)\n",
    "    if col_origin is None or col_sim1 is None or col_sim2 is None:\n",
    "        raise KeyError(\"Required keyword columns not found in dataframe.\")\n",
    "    if col_simscore is None:\n",
    "        raise KeyError(\"sim_score (or similar) column not found in dataframe.\")\n",
    "    \n",
    "    # normalize strings\n",
    "    df = df.copy()\n",
    "    df[\"_origin_n\"] = df[col_origin].apply(norm)\n",
    "    df[\"_sim1_n\"]   = df[col_sim1].apply(norm)\n",
    "    df[\"_sim2_n\"]   = df[col_sim2].apply(norm)\n",
    "    df[\"_sim_score\"] = ensure_numeric(df[col_simscore])\n",
    "    \n",
    "    # detect score scale (0-1 vs 0-100)\n",
    "    max_score = df[\"_sim_score\"].max(skipna=True)\n",
    "    scale = 100.0 if (pd.notna(max_score) and max_score > 1.5) else 1.0\n",
    "    thresholds = [t * scale for t in thresholds_decimals]\n",
    "    \n",
    "    # precompute exact flags\n",
    "    df[\"_match1\"] = (df[\"_origin_n\"].notna() & df[\"_sim1_n\"].notna() & (df[\"_origin_n\"] == df[\"_sim1_n\"]))\n",
    "    df[\"_match2\"] = (df[\"_origin_n\"].notna() & df[\"_sim2_n\"].notna() & (df[\"_origin_n\"] == df[\"_sim2_n\"]))\n",
    "    \n",
    "    rows = []\n",
    "    for t_raw, t_eff in zip(thresholds_decimals, thresholds):\n",
    "        sub = df[df[\"_sim_score\"] >= t_eff]\n",
    "        total = len(sub)\n",
    "        if total == 0:\n",
    "            rows.append({\n",
    "                \"threshold\": f\">= {t_eff:g}\" if scale==100 else f\">= {t_raw:.2f}\",\n",
    "                \"total_rows\": 0,\n",
    "                \"match_sim1_count\": 0, \"match_sim1_rate(%)\": np.nan,\n",
    "                \"match_sim2_count\": 0, \"match_sim2_rate(%)\": np.nan,\n",
    "                \"sim1_false_sim2_true_count\": 0, \"sim1_false_sim2_true_rate(%)\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        m1 = sub[\"_match1\"].sum()\n",
    "        m2 = sub[\"_match2\"].sum()\n",
    "        m1_false_m2_true = ((~sub[\"_match1\"]) & sub[\"_match2\"]).sum()\n",
    "        \n",
    "        rows.append({\n",
    "            \"threshold\": f\">= {int(t_eff)}\" if scale==100 else f\">= {t_raw:.2f}\",\n",
    "            \"total_rows\": int(total),\n",
    "            \"match_sim1_count\": int(m1), \"match_sim1_rate(%)\": round(m1/total*100, 2),\n",
    "            \"match_sim2_count\": int(m2), \"match_sim2_rate(%)\": round(m2/total*100, 2),\n",
    "            \"sim1_false_sim2_true_count\": int(m1_false_m2_true), \"sim1_false_sim2_true_rate(%)\": round(m1_false_m2_true/total*100, 2),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Load files ----------\n",
    "non_nsc_path = \"./concat_non_nsc_plus_keyword_with_top2_keywords.csv\"\n",
    "nsc_path     = \"./concat_nsc_plus_keyword_with_top2_keywords_fixed.csv\"\n",
    "\n",
    "df_non = pd.read_csv(non_nsc_path, encoding=\"utf-8-sig\", low_memory=False)\n",
    "df_nsc = pd.read_csv(nsc_path,     encoding=\"utf-8-sig\", low_memory=False)\n",
    "\n",
    "# ---------- Thresholds (decimals) ----------\n",
    "thresholds_decimals = [0.95, 0.90, 0.85, 0.80, 0.75, 0.70]\n",
    "\n",
    "# ---------- Compute ----------\n",
    "stats_non = compute_stats(df_non, thresholds_decimals)\n",
    "stats_nsc = compute_stats(df_nsc, thresholds_decimals)\n",
    "\n",
    "# save & display\n",
    "out_non = \"./match_stats_non_nsc_by_threshold.csv\"\n",
    "out_nsc = \"./match_stats_nsc_by_threshold.csv\"\n",
    "stats_non.to_csv(out_non, index=False, encoding=\"utf-8-sig\")\n",
    "stats_nsc.to_csv(out_nsc, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "caas_jupyter_tools.display_dataframe_to_user(\"Non-NSC — Threshold match stats (sim_score)\", stats_non)\n",
    "caas_jupyter_tools.display_dataframe_to_user(\"NSC — Threshold match stats (sim_score)\", stats_nsc)\n",
    "\n",
    "out_non, out_nsc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a53487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold ≥</th>\n",
       "      <th>Rows</th>\n",
       "      <th>kw_vs_answer_exact (cnt)</th>\n",
       "      <th>kw_vs_answer_exact (%)</th>\n",
       "      <th>kw_vs_final_exact (cnt)</th>\n",
       "      <th>kw_vs_final_exact (%)</th>\n",
       "      <th>kw_vs_answer_cossim (avg)</th>\n",
       "      <th>kw_vs_final_cossim (avg)</th>\n",
       "      <th>sim_summary_gen_1 (avg)</th>\n",
       "      <th>sim_summary_gen_2 (avg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>90.00</td>\n",
       "      <td>9</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.888</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>78.12</td>\n",
       "      <td>10</td>\n",
       "      <td>31.25</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.854</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>62</td>\n",
       "      <td>44</td>\n",
       "      <td>70.97</td>\n",
       "      <td>10</td>\n",
       "      <td>16.13</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.812</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>58</td>\n",
       "      <td>70.73</td>\n",
       "      <td>10</td>\n",
       "      <td>12.20</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.783</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>92</td>\n",
       "      <td>66</td>\n",
       "      <td>71.74</td>\n",
       "      <td>10</td>\n",
       "      <td>10.87</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.765</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Threshold ≥  Rows  kw_vs_answer_exact (cnt)  kw_vs_answer_exact (%)  \\\n",
       "0           90    10                         9                   90.00   \n",
       "1           85    32                        25                   78.12   \n",
       "2           80    62                        44                   70.97   \n",
       "3           75    82                        58                   70.73   \n",
       "4           70    92                        66                   71.74   \n",
       "\n",
       "   kw_vs_final_exact (cnt)  kw_vs_final_exact (%)  kw_vs_answer_cossim (avg)  \\\n",
       "0                        9                  90.00                      0.923   \n",
       "1                       10                  31.25                      0.899   \n",
       "2                       10                  16.13                      0.878   \n",
       "3                       10                  12.20                      0.871   \n",
       "4                       10                  10.87                      0.876   \n",
       "\n",
       "   kw_vs_final_cossim (avg)  sim_summary_gen_1 (avg)  sim_summary_gen_2 (avg)  \n",
       "0                     0.923                    0.888                      1.0  \n",
       "1                     0.711                    0.854                      1.0  \n",
       "2                     0.650                    0.812                      1.0  \n",
       "3                     0.641                    0.783                      1.0  \n",
       "4                     0.634                    0.765                      1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44986fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
